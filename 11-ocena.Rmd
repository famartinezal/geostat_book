---
knit: bookdown::preview_chapter
---

# Ocena jakości estymacji {#ocena-jakosci-estymacji}

<!--[UWAGA POMIESZANY ME Z RESZTĄ!!]-->

```{r, message=FALSE, warning=FALSE}
library('gstat')
library('sp')
library('ggplot2')
library('caret')
library('geostatbook')
data(punkty)
data(siatka)
```

```{r setup6, echo=FALSE, include=FALSE}
library('knitr')
library('Cairo')
library('grDevices')
knitr::opts_chunk$set(warning=FALSE, message=FALSE, dev = "cairo_pdf")
# options(device = "cairo_pdf")
```

```{r, echo=FALSE}
par(mar=c(rep(0, 4)))
```

## Statystyki jakości estymacji
### Statystyki jakości estymacji

W momencie, gdy trzeba określić jakość estymacji lub porównać wyniki pomiędzy estymacjami należy zastosować tzw. statystyki jakości estymacji. Do podstawowych statystyk ocen jakości estymacji należą:

- Średni błąd predykcji (MPE, ang. *mean prediction error*)
- Pierwiastek średniego błędu kwadratowego (RMSE, ang. *root mean square error*)
- Rozkład błędu (np. 5. percentyl, mediana, 95. percentyl)
    
Idealna estymacja dawałaby brak błędu oraz współczynnik korelacji pomiędzy pomiarami (całą populacją) i szacunkiem równy 1. Wysokie, pojedyncze wartości błędu mogą świadczyć, np. o wystąpieniu wartości odstających.

### Średni błąd estymacji

Średni błąd estymacji (MPE) można wyliczyć korzystając z poniższego wzoru:

$$ MPE=\frac{\sum_{i=1}^{n}(\hat{v}_i-v_i)}{n} $$  

, lub używając funkcji `mean()` w R.

```{r, eval=FALSE}
MPE <- mean(estymacja - obserwowane)
```

Optymalnie wartość średniego błędu estymacji powinna być jak najbliżej 0.

### Pierwiastek średniego błędu kwadratowego

Pierwiastek średniego błędu kwadratowego (RMSE) jest możliwy do wyliczenia poprzez wzór:

$$ RMSE=\sqrt{\frac{\sum_{i=1}^{n}(v_i-\hat{v}_i)^2}{n}} $$     

, jak i proste obliczenie w R.

```{r, eval=FALSE}
RMSE <- sqrt(mean((obserwowane - estymacja)^2))
```

Optymalnie wartość pierwiastka średniego błędu kwadratowego powinna być jak najmniejsza.

### Statystyki jakości estymacji | Mapa

Do oceny przestrzennej jakości predykcji można również zastosować mapę przestawiającą błędy predykcji (błędy estymacji). Wyliczenie błędów estymacji odbywa się poprzez odjęcie od predykcji wartości obserwowanej.

```{r, eval=FALSE}
blad_predykcji <- estymacja - obserwowane
```

```{r mapa, echo=FALSE, message=FALSE}
set.seed(124)
indeks <- as.vector(createDataPartition(punkty$temp, p=0.75, list=FALSE))
train <- punkty[indeks, ]
test <- punkty[-indeks, ]
vario <- variogram(temp~1, data=train)
model <- vgm(10, model = 'Sph', range = 4000, nugget = 0.5)
fitted <- fit.variogram(vario, model)
test_sk <- krige(temp~1, train, test, model=fitted, beta=15.324)
blad_predykcji_sk <-  test_sk$var1.pred - test$temp
test_sk$blad_predykcji_sk <- blad_predykcji_sk
test_sk$true <-  test$temp
spplot(test_sk, 'blad_predykcji_sk', main='Błąd predykcji', colorkey=TRUE)
```

### Statystyki jakości estymacji | Histogram

Błąd predykcji można również przedstawić na wykresach, między innymi, na histogramie.

```{r hist, echo=FALSE}
ggplot(as.data.frame(test_sk), aes(blad_predykcji_sk)) + geom_histogram() + xlab('Błąd predykcji') + ylab('Liczebność')
```

### Statystyki jakości estymacji | Wykres rozrzutu

Do porównania pomiędzy wartością estymowaną a obserwowaną może również posłużyć wykres rozrzutu.

```{r point, echo=FALSE}
ggplot(as.data.frame(test_sk), aes(var1.pred, true)) + geom_point() + xlab('Predykcja') + ylab('Zmierzona wartość')
```

## Walidacja wyników estymacji

### Walidacja wyników estymacji

Dokładne dopasowanie modelu do danych może w efekcie nie dawać najlepszych wyników. Szczególnie będzie to widoczne w przypadku modelowania, w którym dane obarczone są znacznym szumem (zawierają wyraźny błąd) lub też posiadają kilka wartości odstających. W efekcie ważne jest stosowanie metod pozwalających na wybranie optymalnego modelu. Do takich metod należy, między innymi, walidacja podzbiorem (ang. *jackknifing*) oraz kroswalidacja (ang. *crossvalidation*).

### Walidacja podzbiorem 

Walidacja podzbiorem polega na podziale zbioru danych na dwa podzbiory - treningowy i testowy. Zbiór treningowy służy do stworzenia semiwariogramu empirycznego, zbudowania modelu oraz estymacji wartości. Następnie wynik estymacji porównywany jest z rzeczywistymi wartościami ze zbioru testowego. Zaletą tego podejścia jest stosowanie danych niezależnych od estymacji do oceny jakości modelu. Wadą natomiast jest konieczność posiadania (relatywnie) dużego zbioru danych.

Na poniższym przykładzie zbiór danych dzielony jest używając funkcji `createDataPartition()` z pakietu `caret`. Użycie tej funkcji powoduje stworzenie indeksu zawierającego numery wierszy dla zbioru treningowego. Ważną zaletą funkcji `createDataPartition()` jest to, iż  w zbiorze treningowym i testowym zachowane są podobne rozkłady wartości. W przykładzie użyto argumentu `p=0.75`, który oznacza, że 75% danych będzie należało do zbioru treningowego, a 25% do zbioru testowego. Następnie korzystając ze stworzonego indeksu, budowane są dwa zbiory danych - treningowy (`train`) oraz testowy (`test`).

```{r }
set.seed(124)
indeks <- as.vector(createDataPartition(punkty$temp, p=0.75, list=FALSE))
indeks
train <- punkty[indeks, ]
test <- punkty[-indeks, ]
```

Dalszym krokiem jest stworzenie semiwariogramu empirycznego oraz jego modelowanie w oparciu o zbiór treningowy.

```{r}
vario <- variogram(temp~1, data=train)
model <- vgm(10, model = 'Sph', range = 4000, nugget = 0.5)
fitted <- fit.variogram(vario, model)
plot(vario, model=fitted)
```

Do porównania wyników estymacji w stosunku do zbioru testowego posłuży funkcja `krige()`. Wcześniej wymagała ona podania wzoru, zbioru punktowego, siatki oraz modelu. W tym przypadku jednak chcemy porównać wynik estymacji i testowy zbiór punktowy. Dlatego też, zamiast obiektu siatki definiujemy obiekt zawierający zbiór testowy (`test`).

```{r}
test_sk <- krige(temp~1, train, test, model=fitted, beta=15.324)
summary(test_sk)
```

Uzyskane w ten sposób wyniki możemy określić używając statystyk jakości estymacji lub też wykresów.

```{r}
blad_predykcji_sk <-  test_sk$var1.pred - test$temp
summary(blad_predykcji_sk)

MPE <- mean(test_sk$var1.pred - test$temp)
MPE

RMSE <- sqrt(mean((test$temp-test_sk$var1.pred)^2))
RMSE

test_sk$blad_predykcji_sk <- blad_predykcji_sk
spplot(test_sk, 'blad_predykcji_sk')
```

W sytuacji, gdy uzyskany model jest wystarczająco dobry, możemy również uzyskać estymację dla całego obszaru z użyciem funkcji `krige()`, tym razem jednak podając obiekt siatki.

```{r}
test_sk <- krige(temp~1, train, siatka, model=fitted, beta=15.324)
spplot(test_sk, 'var1.pred')
```

### Kroswalidacja

W przypadku kroswalidacji te same dane wykorzystywane są do budowy modelu, estymacji, a następnie do oceny prognozy. Procedura kroswalidacji LOO (ang. *leave-one-out cross-validation*) składa się z poniższych kroków:

1. Zbudowanie matematycznego modelu z dostępnych obserwacji
2. Dla każdej znanej obserwacji następuje:
    - Usunięcie jej ze zbioru danych
    - Użycie modelu do wykonania predykcji w miejscu tej obserwacji
    - Wyliczenie reszty (ang. *residual*), czyli różnicy pomiędzy znaną wartością a estymacją
3. Podsumowanie otrzymanych wyników
    
W pakiecie `gstat`, kroswalidacja LOO jest dostępna w funkcjach `krige.cv()` oraz `gstat.cv()`. Działają one bardzo podobnie jak funkcje `krige()` oraz `gstat()`, jednak w przeciwieństwie do nich nie wymagają podania obiektu siatki.

```{r loovv, cache=TRUE}
vario <- variogram(temp~1, data=punkty)
model <- vgm(10, model = 'Sph', range = 4000, nugget = 0.5)
fitted <- fit.variogram(vario, model)

cv_sk <- krige.cv(temp~1, punkty, model=fitted, beta=15.324, verbose=FALSE)
summary(cv_sk)
spplot(cv_sk, 'residual')
```

Podobnie jak w walidacji podzbiorem, gdy uzyskany model jest wystarczająco dobry, estymację dla całego obszaru uzyskuje się z użyciem funkcji `krige()`.

```{r}
cv_skk <- krige(temp~1, train, siatka, model=fitted, beta=15.324)
spplot(cv_skk, 'var1.pred')
```

<!-- 

```{r, eval=FALSE}
# ok_loocv <- krige.cv(temp~1, punkty, model=model_zl2)
# summary(ok_loocv)
```

- Tutaj inne przykłady
- Wykresy z loocv
- wykresy porównujące

```{r, eval=FALSE}
# ok_fit <- gstat(formula=temp~1, data=punkty, model=model_zl2)
# ok_loocv <- gstat.cv(OK_fit, debug.level=0, random=FALSE)
# spplot(pe[6])
```

## 
- prezentacja 5 Ani
- spatinter folder
- AIC

## Walidacja wyników estymacji

### Walidacja wyników estymacji |  Kriging zwykły - LOO crossvalidation
krige.cv

```{r, eval=FALSE }
# OK_fit <- gstat(id="OK_fit", formula=temp~1, data=punkty, model=fitted)
# pe <- gstat.cv(OK_fit, debug.level=0, random=FALSE)
# spplot(pe[6])
# 
# z <- predict(OK_fit, newdata = grid, debug.level = 0)
# grid2 <- grid
# grid2$OK_pred <- z$OK_fit.pred
# grid2$OK_se <- sqrt(z$OK_fit.var)
# library('rasterVis')
# spplot(grid2, 'OK_pred')
# spplot(grid2, 'OK_se')
```

### Walidacja wyników estymacji |  K  Kriging uniwersalny - LOO crossvalidation

```{r, eval=FALSE }
# KU_fit <- gstat(id="KU_fit", formula=temp~odl_od_morza, data=punkty, model=fitted_ku)
# pe <- gstat.cv(KU_fit, debug.level=0, random=FALSE)
# spplot(pe[6])

# dodanie odległości od morza do siatki !!
# z_KU <- predict(KU_fit, newdata = grid, debug.level = 0)
# grid$KU_pred <- z$KU_fit.pred
# grid$KU_se <- sqrt(z$KU_fit.var)
# library('rasterVis')
# spplot(grid, 'KU_pred')
# spplot(grid, 'KU_se')
```
-->
